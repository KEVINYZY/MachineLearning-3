{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA模型应用：一眼看穿希拉里的邮件\n",
    "我们拿到希拉里泄露的邮件，跑一把LDA，看看她平时都在聊什么。\n",
    "首先，导入我们需要的一些库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ExtractedBodyText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B6\\nThursday, March 3, 2011 9:45 PM\\nH: Latest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Thx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>H &lt;hrod17@clintonemail.com&gt;\\nFriday, March 11,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>Pis print.\\n-•-...-^\\nH &lt; hrod17@clintonernail...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>H &lt;hrod17@clintonemail.corn&gt;\\nFriday, March 11...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id                                  ExtractedBodyText\n",
       "1   2  B6\\nThursday, March 3, 2011 9:45 PM\\nH: Latest...\n",
       "2   3                                                Thx\n",
       "4   5  H <hrod17@clintonemail.com>\\nFriday, March 11,...\n",
       "5   6  Pis print.\\n-•-...-^\\nH < hrod17@clintonernail...\n",
       "7   8  H <hrod17@clintonemail.corn>\\nFriday, March 11..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "df = pd.read_csv(\"/home/ubuntu/mydata/HillaryEmails.csv\")\n",
    "# 原邮件数据中有很多Nan的值，直接扔了。\n",
    "df = df[['Id','ExtractedBodyText']].dropna()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_email_text(text):\n",
    "    text = text.replace('\\n',\" \") #新行，我们是不需要的\n",
    "    text = re.sub(r\"-\", \" \", text) #把 \"-\" 的两个单词，分开。（比如：july-edu ==> july edu）\n",
    "    text = re.sub(r\"\\d+/\\d+/\\d+\", \"\", text) #日期，对主体模型没什么意义\n",
    "    text = re.sub(r\"[0-2]?[0-9]:[0-6][0-9]\", \"\", text) #时间，没意义\n",
    "    text = re.sub(r\"[\\w]+@[\\.\\w]+\", \"\", text) #邮件地址，没意义\n",
    "    text = re.sub(r\"/[a-zA-Z]*[:\\//\\]*[A-Za-z0-9\\-_]+\\.+[A-Za-z0-9\\.\\/%&=\\?\\-_]+/i\", \"\", text) #网址，没意义\n",
    "    pure_text = ''\n",
    "    # 以防还有其他特殊字符（数字）等等，我们直接把他们loop一遍，过滤掉\n",
    "    for letter in text:\n",
    "        # 只留下字母和空格\n",
    "        if letter.isalpha() or letter==' ':\n",
    "            pure_text += letter\n",
    "    # 再把那些去除特殊字符后落单的单词，直接排除。\n",
    "    # 我们就只剩下有意义的单词了。\n",
    "    text = ' '.join(word for word in pure_text.split() if len(word)>1)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 'Thursday March PM Latest How Syria is aiding Qaddafi and more Sid hrc memo syria aiding libya docx hrc memo syria aiding libya docx March For Hillary',\n",
       "       'Thx',\n",
       "       'Friday March PM Huma Abedin Fw Latest How Syria is aiding Qaddafi and more Sid hrc memo syria aiding libya docx Pis print'], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = df['ExtractedBodyText']\n",
    "docs = docs.apply(lambda s: clean_email_text(s)) \n",
    "doclist = docs.values\n",
    "docs.head(3).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA模型构建：\n",
    "好，我们用Gensim来做一次模型构建\n",
    "首先，我们得把我们刚刚整出来的一大波文本数据\n",
    "[[一条邮件字符串]，[另一条邮件字符串], ...]\n",
    "转化成Gensim认可的语料库形式：\n",
    "[[一，条，邮件，在，这里],[第，二，条，邮件，在，这里],[今天，天气，肿么，样],...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['thursday',\n",
       " 'march',\n",
       " 'pm',\n",
       " 'latest',\n",
       " 'how',\n",
       " 'syria',\n",
       " 'is',\n",
       " 'aiding',\n",
       " 'qaddafi',\n",
       " 'and',\n",
       " 'more',\n",
       " 'sid',\n",
       " 'hrc',\n",
       " 'memo',\n",
       " 'syria',\n",
       " 'aiding',\n",
       " 'libya',\n",
       " 'docx',\n",
       " 'hrc',\n",
       " 'memo',\n",
       " 'syria',\n",
       " 'aiding',\n",
       " 'libya',\n",
       " 'docx',\n",
       " 'march',\n",
       " 'for',\n",
       " 'hillary']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from gensim import corpora, models, similarities\n",
    "import gensim\n",
    "eng_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "text=[[word for word in doc.lower().split() if word not in eng_stopwords] for doc in doclist]\n",
    "text[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 建立语料库\n",
    "用词袋的方法，把每个单词用一个数字index指代，并把我们的原文本变成一条长长的数组："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(33, 1), (591, 1), (592, 1), (593, 1), (594, 1), (595, 1)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary = corpora.Dictionary(text)\n",
    "corpus = [dictionary.doc2bow(word) for word in text]\n",
    "corpus[13]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个列表告诉我们，第14（从0开始是第一）个邮件中，一共6个有意义的单词（经过我们的文本预处理，并去除了停止词后）\n",
    "其中，36号单词出现1次，505号单词出现1次，以此类推。。。\n",
    "接着，我们终于可以建立模型了："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.040*\"the\" + 0.035*\"and\" + 0.022*\"of\" + 0.021*\"to\" + 0.017*\"in\"'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=20)\n",
    "#我们可以看到，第10号分类，其中最常出现的单词是：\n",
    "lda.print_topic(10, topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, '0.028*\"to\" + 0.025*\"the\" + 0.017*\"and\" + 0.016*\"of\" + 0.010*\"state\"'),\n",
       " (1,\n",
       "  '0.094*\"fyi\" + 0.017*\"richards\" + 0.016*\"and\" + 0.010*\"see\" + 0.009*\"below\"'),\n",
       " (2, '0.044*\"the\" + 0.026*\"of\" + 0.026*\"to\" + 0.022*\"and\" + 0.017*\"in\"'),\n",
       " (3, '0.019*\"the\" + 0.015*\"and\" + 0.012*\"in\" + 0.011*\"to\" + 0.011*\"on\"'),\n",
       " (4,\n",
       "  '0.024*\"in\" + 0.019*\"part\" + 0.019*\"the\" + 0.019*\"release\" + 0.018*\"yes\"'),\n",
       " (5, '0.024*\"ok\" + 0.023*\"the\" + 0.020*\"of\" + 0.010*\"in\" + 0.010*\"pls\"'),\n",
       " (6,\n",
       "  '0.085*\"pm\" + 0.066*\"am\" + 0.043*\"office\" + 0.037*\"secretarys\" + 0.023*\"meeting\"'),\n",
       " (7, '0.043*\"the\" + 0.030*\"to\" + 0.028*\"and\" + 0.020*\"you\" + 0.016*\"we\"'),\n",
       " (8, '0.076*\"the\" + 0.034*\"of\" + 0.033*\"to\" + 0.031*\"and\" + 0.024*\"in\"'),\n",
       " (9, '0.036*\"pm\" + 0.022*\"re\" + 0.022*\"am\" + 0.022*\"fw\" + 0.015*\"to\"'),\n",
       " (10, '0.040*\"the\" + 0.035*\"and\" + 0.022*\"of\" + 0.021*\"to\" + 0.017*\"in\"'),\n",
       " (11, '0.035*\"to\" + 0.023*\"will\" + 0.021*\"we\" + 0.018*\"on\" + 0.015*\"the\"'),\n",
       " (12, '0.056*\"the\" + 0.037*\"to\" + 0.023*\"and\" + 0.020*\"in\" + 0.017*\"that\"'),\n",
       " (13, '0.022*\"pm\" + 0.019*\"am\" + 0.017*\"of\" + 0.017*\"state\" + 0.013*\"to\"'),\n",
       " (14, '0.020*\"to\" + 0.012*\"no\" + 0.012*\"doc\" + 0.011*\"and\" + 0.010*\"the\"'),\n",
       " (15, '0.057*\"the\" + 0.031*\"of\" + 0.027*\"and\" + 0.020*\"to\" + 0.020*\"that\"'),\n",
       " (16,\n",
       "  '0.033*\"the\" + 0.019*\"in\" + 0.014*\"of\" + 0.014*\"percent\" + 0.013*\"bloomberg\"'),\n",
       " (17, '0.028*\"is\" + 0.020*\"the\" + 0.015*\"to\" + 0.015*\"and\" + 0.014*\"it\"'),\n",
       " (18, '0.043*\"you\" + 0.042*\"to\" + 0.023*\"for\" + 0.018*\"and\" + 0.014*\"me\"'),\n",
       " (19, '0.030*\"the\" + 0.026*\"and\" + 0.022*\"to\" + 0.022*\"you\" + 0.019*\"for\"')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#我们把所有的主题打印出来看看\n",
    "lda.print_topics(num_topics=20, num_words=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

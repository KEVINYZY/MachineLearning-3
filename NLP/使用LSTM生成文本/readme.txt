1.这里用温斯顿丘吉尔的人物传记作为我们的学习语料。
(各种中文语料可以自行网上查找， 英文的小说语料可以从古登堡计划网站下载txt平文本：https://www.gutenberg.org/wiki/Category:Bookshelf)

2.功能:给了前置的字母以后，预测下一个字母

3.将文本中的所有的字符生成一个list，共60个元素
  将这个list中的每一个元素依据对应的索引生成一个index：char和char：index的字典

4.每次取出文本中的101个，前100个将其查询字典找出对应的数字，生成向量，作为特征x
  第101个也转为对应的数字作为标签y

5.将x按照[样本数，时间步伐，特征]的格式进行reshape，并将每个数据压缩到0到1之间

6.将y转成one-hot向量(否则例如y为31，需要预测31到32之间的，比较困难)

7.设置LSTM模型，LSM层128个神经元，输入维度为[100,1],再接一个全连接神经元，输出维度为60，激活函数为softmax
  设置损失函数为categorical_crossentropy，梯度下降为adam
  模型nb_epoch=10, batch_size=32，一共跑10圈，每次的batch放32个数据集

8.将预测出来的字符放回原字符串，继续预测下一个，然后形成一篇文章


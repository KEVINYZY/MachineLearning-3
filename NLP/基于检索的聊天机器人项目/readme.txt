1.数据:Ubuntu对话数据集，来自Ubuntu的IRC网络上的对话日志
https://drive.google.com/open?id=0B_bZck-ksdkpVEtVc1R6Y01HMWM

2.匹配:Q1 A1作为输入，匹配为1，不匹配为0

3.输入:分词之后，以词嵌入作为输入

4.损失函数:交叉熵

5.LSTM网络:Net.PNG,其中隐向量C为256*1的向量，代表问题，隐向量R代表256*1的答案向量，M代表256*256的参数矩阵，需要去学习

	   最后得到一共1*1的值，通过sigmoid转化为概率

	   注意点:图片中LSTM有2组，实际只需要一组即可，因为都是文本信息的抽取，此处2组是为了方便理解

 6.环境:Tensorflow

 7.#### Training

	```
	python udc_train.py
	```


	#### Evaluation

	```
	python udc_test.py --model_dir=...
	```


	#### Evaluation

	```
	python udc_predict.py --model_dir=...
	```

8.详细步骤:
  1.数据说明:训练数据包含100w的例子，一半是正面（ｌａｂｅｌ＝１）一半是负面（ｌａｂｌｅ＝０）,每个例子包含一个上下文，一个正面的标注意味着答复是符合上下文的，负面的标注意味着答复不符合（答复是从其他语料对里去随机挑选搭配过来的）

   语料处理包括（分词，提取词干，词意恢复）,脚本也做了把名字、地点、组织、URL。系统路径等实体信息用特殊的token来替代

    每个测试验证集记录包含上下文、正确答复和9个不正确的干扰项。模型的目标是给争取答复打上高分，给干扰项目打上低分.

  2.prepare_data.py:将train,test，valid数据集转化为tfrecords格式的文件，同时生产词汇表(一行一个单词)，保存glove模型.TFRecords文件包含了tf.train.Example 协议内存块(protocol buffer)(协议内存块包含了字段 Features)

    TFRecord文件存储的是词语对应的数字，每个例子包含下面的字段：1、上下文（Q）：代表上下文文本的一系列词id序列例如[231,2190,737,0,912] 2、上下文长度 3、回复 ：同1一样的词id序列 4、回复长度 5、标注：训练数据里才有 0 or 1

    6、干扰项[N] 测试/验证集里有，N 属于0-8.都是词id的序列 7、干扰项[N]长度 

  3.udc_input.py:读取TFRecord文件，获取feature

  4.udc_hparams.py:定义一些超参数，比如rnn_dim，embedding_dim，vocab_path

  5.dual_encoder.py：
    训练:
     (1).获取词汇表，以及每一个词汇对应的索引 
     (2).从glove中获取每一个word的对应的向量。 
     (3).将获取到的每一个word的向量，按照词汇表里的索引的顺序，保存在矩阵里。这样一来实现了将词汇表转化为向量矩阵
     (4).将问题和答案查找到向量，通过同一组RNN，在结果处分开(节约时间),得到二个255×1的向量
    预测:
     (1)(c * M) * r（c转制),其中c×M得到一个1×255的向量，当成是生成的回复r1。将r1与r做乘法，然后通过sigmoid转化为概率，M需要在训练中学习。y等于1说明是正确答复，0代表错误答复。损失函数为交叉商
    测试：
      K召回，它的意思是让模型在10个备选答案（1个正确答案和9个干扰项）里找出最好的K个答案，如果正确的那个答案在K个答案里，就标记为成功案例.
      一个问题输出与10个答案均会得到一个概率值，假如k等于2,那么取概率最高的2个答案，如果正确答案在里面就认为是是成功案例
      



